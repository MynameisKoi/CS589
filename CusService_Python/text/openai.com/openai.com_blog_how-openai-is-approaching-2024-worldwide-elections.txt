


How OpenAI is approaching 2024 worldwide elections













CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit BlogHow OpenAI is approaching 2024 worldwide electionsWeâre working to prevent abuse, provide transparency on AI-generated content, and improve access to accurate voting information.Illustration: Justin Jay Wang Ã DALLÂ·EJanuary 15, 2024AuthorsOpenAI Safety & AlignmentProtecting the integrity of elections requires collaboration from every corner of the democratic process, and we want to make sure our technology is not used in a way that could undermine this process.Â Our tools empower people to improve their daily lives and solve complex problemsâfrom using AI to enhance state services to simplifying medical forms for patients.We want to make sure that our AI systems are built, deployed, and used safely. Like any new technology, these tools come with benefits and challenges. They are also unprecedented, and we will keep evolving our approach as we learn more about how our tools are used.As we prepare for elections in 2024 across the worldâs largest democracies, our approach is to continue our platform safety work by elevating accurate voting information, enforcing measured policies, and improving transparency. We have a cross-functional effort dedicated to election work, bringing together expertise from our safety systems, threat intelligence, legal, engineering, and policy teams to quickly investigate and address potential abuse.Â The following are key initiatives our teams are investing in to prepare for elections this year:Preventing abuseWe expect and aim for people to use our tools safely and responsibly, and elections are no different. We work to anticipate and prevent relevant abuseâsuch as misleading âdeepfakesâ, scaled influence operations, or chatbots impersonating candidates. Prior to releasing new systems, we red team them, engage users and external partners for feedback, and build safety mitigations to reduce the potential for harm. For years, weâve been iterating on tools to improve factual accuracy, reduce bias, and decline certain requests. These tools provide a strong foundation for our work around election integrity. For instance, DALLÂ·E has guardrails to decline requests that ask for image generation of real people, including candidates.We regularly refine our Usage Policies for ChatGPT and the API as we learn more about how people use or attempt to abuse our technology. A few to highlight for elections:Â Weâre still working to understand how effective our tools might be for personalized persuasion. Until we know more, we donât allow people to build applications for political campaigning and lobbying.Â People want to know and trust that they are interacting with a real person, business, or government. For that reason, we donât allow builders to create chatbots that pretend to be real people (e.g., candidates) or institutions (e.g., local government).Â We donât allow applications that deter people from participation in democratic processesâfor example, misrepresenting voting processes and qualifications (e.g., when, where, or who is eligible to vote) or that discourage voting (e.g., claiming a vote is meaningless).With our new GPTs, users can report potential violations to us.With our new GPTs, users can report potential violations to us.Transparency around AI-generated contentBetter transparency around image provenanceâincluding the ability to detect which tools were used to produce an imageâcan empower voters to assess an image with trust and confidence in how it was made. Weâre working on several provenance efforts. Early this year, we will implement the Coalition for Content Provenance and Authenticityâs digital credentialsâan approach that encodes details about the contentâs provenance using cryptographyâfor images generated by DALLÂ·E 3.Â We are also experimenting with a provenance classifier, a new tool for detecting images generated by DALLÂ·E. Our internal testing has shown promising early results, even where images have been subject to common types of modifications. We plan to soon make it available to our first group of testersâincluding journalists, platforms, and researchersâfor feedback. Finally, ChatGPT is increasingly integrating with existing sources of informationâfor example, users will start to get access to real-time news reporting globally, including attribution and links. Transparency around the origin of information and balance in news sources can help voters better assess information and decide for themselves what they can trust.Improving access to authoritative voting informationIn the United States, we are working with the National Association of Secretaries of State (NASS), the nation's oldest nonpartisan professional organization for public officials. ChatGPT will direct users to CanIVote.org, the authoritative website on US voting information, when asked certain procedural election related questionsâfor example, where to vote. Lessons from this work will inform our approach in other countries and regions.Â Weâll have more to share in the coming months. We look forward to continuing to work with and learn from partners to anticipate and prevent potential abuse of our tools in the lead up to this yearâs global elections.AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
